{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import nltk.classify\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords as stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = 'C:/Users/shara/Documents/SentenceCorpus/unlabeled_articles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_from_directory(directory):\n",
    "    \"\"\"Lists all file paths from given directory\"\"\"\n",
    "\n",
    "    ret_val = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            ret_val.append(str(directory) + \"/\" + str(file))\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_paths = list_files_from_directory(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate10_7_1.txt', 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate10_7_2.txt', 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate10_7_3.txt', 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate1_13_1.txt', 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate1_13_2.txt', 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate1_13_3.txt', 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate2_66_1.txt', 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate2_66_2.txt', 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate2_66_3.txt', 'C:/Users/shara/Documents/SentenceCorpus/labeled_articles//arxiv_annotate3_80_1.txt']\n"
     ]
    }
   ],
   "source": [
    "print(train_file_paths[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\"Reads all lines from file on given path\"\"\"\n",
    "\n",
    "    f = open(path, \"r\", errors='ignore')\n",
    "    read = f.readlines()\n",
    "    ret_val = []\n",
    "    for line in read:\n",
    "        if line.startswith(\"#\"):\n",
    "            pass\n",
    "        else:\n",
    "            ret_val.append(line)\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MISC\\talthough the internet as level topology has been extensively studied over the past few years  little is known about the details of the as taxonomy\\n',\n",
       " 'MISC\\tan as  node  can represent a wide variety of organizations  e g   large isp  or small private business  university  with vastly different network characteristics  external connectivity patterns  network growth tendencies  and other properties that we can hardly neglect while working on veracious internet representations in simulation environments\\n',\n",
       " 'AIMX\\tin this paper  we introduce a radically new approach based on machine learning techniques to map all the ases in the internet into a natural as taxonomy\\n',\n",
       " 'OWNX\\twe successfully classify  NUMBER   NUMBER  percent  of ases with expected accuracy of  NUMBER   NUMBER  percent \\n',\n",
       " 'OWNX\\twe release to the community the as level topology dataset augmented with   NUMBER   the as taxonomy information and  NUMBER   the set of as attributes we used to classify ases\\n',\n",
       " 'OWNX\\twe believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the internet\\n',\n",
       " 'MISC\\tthe rapid expansion of the internet in the last two decades has produced a large scale system of thousands of diverse  independently managed networks that collectively provide global connectivity across a wide spectrum of geopolitical environments\\n',\n",
       " 'MISC\\tfrom  NUMBER  to  NUMBER  the number of globally routable as identifiers has increased from less than  NUMBER   NUMBER  to more than  NUMBER   NUMBER   exerting significant pressure on interdomain routing as well as other functional and structural parts of the internet\\n',\n",
       " 'MISC\\tthis impressive growth has resulted in a heterogenous and highly complex system that challenges accurate and realistic modeling of the internet infrastructure\\n',\n",
       " 'MISC\\tin particular  the as level topology is an intermix of networks owned and operated by many different organizations  e g   backbone providers  regional providers  access providers  universities and private companies\\n',\n",
       " 'MISC\\tstatistical information that faithfully characterizes different as types is on the critical path toward understanding the structure of the internet  as well as for modeling its topology and growth\\n',\n",
       " 'MISC\\tin topology modeling  knowledge of as types is mandatory for augmenting synthetically constructed or measured as topologies with realistic intra as and inter as router level topologies\\n',\n",
       " 'MISC\\tfor example  we expect the network of a dual homed university to be drastically different from that of a dual homed small company\\n',\n",
       " 'MISC\\tthe university will likely contain dozens of internal routers  thousands of hosts  and many other network elements  switches  servers  firewalls \\n',\n",
       " 'MISC\\ton the other hand  the small company will most probably have a single router and a simple network topology\\n',\n",
       " 'MISC\\tsince there is such a diversity among different network types  we cannot accurately augment the as level topology with appropriate router level topologies if we cannot MISC\\tcharacterize the composing ases\\n',\n",
       " 'MISC\\tmoreover  annotating the ases in the as topology with their types is a prerequisite for modeling the evolution of the internet  since different types of ases exhibit different growth patterns\\n',\n",
       " 'MISC\\tfor example  internet service providers  isp  grow by attracting new customers and by engaging in business agreements with other isps\\n',\n",
       " 'MISC\\ton the other hand  small companies that connect to the internet through one or few isps do not grow significantly over time\\n',\n",
       " 'MISC\\tthus  categorizing different types of ases in the internet is necessary to identify network evolution patterns and develop accurate evolution models\\n',\n",
       " 'MISC\\tan as taxonomy is also necessary for mapping ip addresses to different types of users\\n',\n",
       " 'MISC\\tfor example  in traffic analysis studies its often required to distinguish between packets that come from home and business users\\n',\n",
       " 'MISC\\tgiven an as taxonomy  its possible to realize this goal by checking the type of as that originates the prefix in which an ip address lies\\n',\n",
       " 'AIMX\\tin this work  we introduce a radically new approach based on machine learning to construct a representative as taxonomy\\n',\n",
       " 'OWNX\\twe develop an algorithm to classify ases based on empirically observed differences between as characteristics\\n',\n",
       " 'OWNX\\twe use a large set of data from the internet routing registries  irr   CITATION  and from routeviews  CITATION  to identify intrinsic differences between ases of different types\\n',\n",
       " 'AIMX\\tthen  we employ a novel machine learning technique to build a classification algorithm that exploits these differences to classify ases into six representative classes that reflect ases with different network properties and infrastructures\\n',\n",
       " 'OWNX\\twe derive macroscopic statistics on the different types of ases in the internet and validate our results using a sample of  NUMBER  manually identified as types\\n',\n",
       " 'OWNX\\tour validation demonstrates that our classification algorithm achieves high accuracy   NUMBER   NUMBER  percent  of the examined classifications were correct\\n',\n",
       " 'OWNX\\tfinally  we make our results and our classifier publicly available to promote further research and understanding of the internet s structure and evolution\\n',\n",
       " 'OWNX\\tin section  we start with a brief discussion of related work\\n',\n",
       " 'OWNX\\tsection  describes the data we used  and in section  we specify the set of as classes we use in our experiments\\n',\n",
       " 'OWNX\\tsection  introduces our classification approach and results\\n',\n",
       " 'OWNX\\twe validate them in section  and conclude in section \\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file('C:/Users/shara/Documents/SentenceCorpus/labeled_articles/arxiv_annotate1_13_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fitness functions based on test cases are very common in Genetic Programming (GP)\\n',\n",
       " 'This process can be assimilated to a learning task, with the inference of models from a limited number of samples\\n',\n",
       " 'This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions\\n',\n",
       " 'Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced\\n',\n",
       " 'GP is particularly suited for problems that can be assimilated to learning tasks, with the minimization of the error between the obtained and desired outputs for a limited number of test cases -- the training data, using a ML terminology\\n',\n",
       " 'Indeed, the classical GP examples of symbolic regression, boolean multiplexer and artificial ant  CITATION  are only simple instances of well-known learning problems (i e respectively regression, binary classification and reinforcement learning)\\n',\n",
       " 'In the early years of GP, these problems were tackled using a single data set, reporting results on the same data set that was used to evaluate the fitnesses during the evolution\\n',\n",
       " 'This was justifiable by the fact that these are toy problems used only to illustrate the potential of GP\\n',\n",
       " 'In the ML community, it is recognized that such methodology is flawed, given that the learning algorithm can overfit the data used during the training and perform poorly on unseen data of the same application domain  CITATION\\n',\n",
       " 'Hence, it is important to report results on a set of data that was not used during the learning stage\\n',\n",
       " \"This is what we call in this paper a  two data sets methodology , with a training set used by the learning algorithm and a test set used to report the performance of the algorithm on unseen data, which is a good indicator of the algorithm's generalization (or robustness) capability\\n\",\n",
       " 'Even though this methodology has been widely accepted and applied in the ML and PR communities for a long time, the EC community still lags behind by publishing papers that are reporting results on data sets that were used during the evolution (training) phase\\n',\n",
       " 'This methodological problem has already been spotted (see  CITATION ) and should be less and less common in the future\\n',\n",
       " 'The two data sets methodology prevents reporting flawed results of learning algorithms that overfit the training set\\n',\n",
       " 'But this does not prevent by itself overfitting the training set\\n',\n",
       " 'A common approach is to add a third data set -- the validation set -- which helps the learning algorithm to measure its generalization capability\\n',\n",
       " 'This validation set is useful to interrupt the learning algorithm when overfitting occurs and/or select a configuration of the learning machine that maximizes the generalization performances\\n',\n",
       " 'This third data set is commonly used to train classifiers such as back-propagation neural networks and can be easily applied to EC-based learning\\n',\n",
       " 'But this approach has an important drawback: it removes a significant amount of data from the training set, which can be harmful to the learning process\\n',\n",
       " 'Indeed, the richer the training set, the more representative it can be of the real data distribution, and the more the learning algorithm can be expected to converge toward robust solutions\\n',\n",
       " 'In the light of these considerations, an objective of this paper is to investigate the effect of a validation set to select the best-of-run individuals for a GP-based learning application\\n',\n",
       " 'Another concern of the ML and PR communities is to develop learning algorithms that generate simple solutions\\n',\n",
       " \"An argument behind this is the Occam's Razor principle, which states that between solutions of comparable quality, the simplest solutions must be preferred\\n\",\n",
       " \"Another argument is the minimum description length principle  CITATION , which states that the ``best'' model is the one that minimizes the amount of information needed to encode the model and the data given the model\\n\",\n",
       " 'Preference for simpler solutions and overfitting avoidance are closely related: it is more likely that a complex solution incorporates specific information from the training set, thus overfitting the training set, compared to a simpler solution\\n',\n",
       " 'But, as mentioned in  CITATION , this argumentation should be taken with care as too much emphasis on minimizing complexity can prevent the discovery of more complex yet more accurate solutions\\n',\n",
       " 'There is a strong link between the minimization of complexity in GP-based learning and the control of code bloat  CITATION , that is an exaggerated growth of program size in the course of GP runs\\n',\n",
       " 'Even though complexity and code bloat are not exactly the same phenomenon, as some kind of bloat is generated by neutral pieces of code that have no effect on the actual complexity of the solutions, most of the mechanisms proposed to control it  CITATION  can also be used to minimize the complexity of solutions obtained by GP-based learning\\n',\n",
       " 'This paper is a study of GP viewed as a learning algorithm\\n',\n",
       " 'More specifically, we investigate two techniques to increase the generalization performance and decrease the complexity of the models: 1) use of a validation set to select best-of-run individuals that generalize well, and 2) use of lexicographic parsimony pressure  CITATION  to reduce the complexity of the generated models\\n',\n",
       " 'These techniques are tested using a GP encoding for binary classification problems, with vectors taken from the learning sets as terminals, and mathematical operations to manipulate these vectors as branches\\n',\n",
       " 'This approach is tested on six different data sets from the UCI ML repository  CITATION\\n',\n",
       " 'Even if the proposed techniques are tested in a specific context, we argue that they can be extended to the frequent situations where GP is used as a learning algorithm\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file('C:/Users/shara/Documents/SentenceCorpus/unlabeled_articles/arxiv_unlabeled/1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading stopwords\n",
    "input_stopwords = read_file(\"C:/Users/shara/Documents/SentenceCorpus/word_lists/stopwords.txt\")\n",
    "stopwords = []\n",
    "for word in input_stopwords:\n",
    "    if word.endswith('\\n'):\n",
    "        word = word[:-1]\n",
    "        stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join(word for word in tokens if word not in stopwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    \"\"\"Returns sentence category and sentence in given line\"\"\"\n",
    "\n",
    "    if \"\\t\" in line:\n",
    "        splits = line.split(\"\\t\")\n",
    "        s_category = splits[0]\n",
    "        sentence = splits[1].lower()\n",
    "        sentence = remove_stopwords(sentence)\n",
    "        pattern = re.compile(\"[^\\w']\")\n",
    "        sentence = pattern.sub(' ', sentence)\n",
    "        sentence = re.sub(' +', ' ', sentence)\n",
    "        return s_category, sentence\n",
    "    else:\n",
    "        splits = line.split(\" \")\n",
    "        s_category = splits[0]\n",
    "        sentence = line[len(s_category)+1:].lower()\n",
    "        sentence = remove_stopwords(sentence)\n",
    "        pattern = re.compile(\"[^\\w']\")\n",
    "        sentence = pattern.sub(' ', sentence)\n",
    "        sentence = re.sub(' +', ' ', sentence)\n",
    "        return s_category, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join(word for word in tokens if word not in stopwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_files_from_directory(directory):\n",
    "    \"\"\"Lists all file paths from given directory\"\"\"\n",
    "\n",
    "    ret_file = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            ret_file.append(file)\n",
    "    return ret_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = 'C:/Users/shara/Documents/SentenceCorpus/unlabeled_articles/'\n",
    "\n",
    "def test_data(folder):\n",
    "    text_data = []\n",
    "    ret_val = []\n",
    "    files_dir = []\n",
    "    for file in os.listdir(test_folder):\n",
    "        ret_val.append(file)\n",
    "\n",
    "    for folder in ret_val:\n",
    "        directory = str(test_folder) + str(folder) + '/'\n",
    "        try:\n",
    "            file_list = test_files_from_directory(str(directory))\n",
    "            for file in file_list:\n",
    "                file_path = str(directory) + str(file)\n",
    "                files_dir.append(file_path)\n",
    "        except:\n",
    "            pass\n",
    "    for path in files_dir:\n",
    "        lines = read_file(path)\n",
    "        for text in lines:\n",
    "            text = text.lower()\n",
    "            text = remove_stopwords(text)\n",
    "            pattern = re.compile(\"[^\\w']\")\n",
    "            text = pattern.sub(' ', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            text_data.append(text)\n",
    "    text_data = pd.DataFrame(text_data, columns=['text'])\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_data(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fitness functions based test cases very common...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this process assimilated learning task inferen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this paper investigation two methods improve g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>results using gp binary classification setup s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gp particularly suited problems assimilated le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  fitness functions based test cases very common...\n",
       "1  this process assimilated learning task inferen...\n",
       "2  this paper investigation two methods improve g...\n",
       "3  results using gp binary classification setup s...\n",
       "4  gp particularly suited problems assimilated le..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(input_folder):\n",
    "    \"\"\"Writes training data from given folder into formatted CSV file\"\"\"\n",
    "\n",
    "    tr_folder = list_files_from_directory(input_folder)\n",
    "    #     all_json = ''\n",
    "    training_dict = {}\n",
    "    label = []\n",
    "    text = []\n",
    "    for file in tr_folder:\n",
    "        lines = read_file(file)\n",
    "        for line in lines:\n",
    "            c, s = process_line(line)\n",
    "            if s.endswith('\\n'):\n",
    "                s = s[:-1]\n",
    "            label.append(c)\n",
    "            text.append(s)\n",
    "        training_dict['text'] = text\n",
    "        training_dict['label'] = label\n",
    "    df = pd.DataFrame(training_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = create_df('C:/Users/shara/Documents/SentenceCorpus/labeled_articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>minimum description length principle online se...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if underlying model class discrete then total ...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdl general one only loss bounds finite but ex...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we show this even case if model class contains...</td>\n",
       "      <td>AIMX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we derive new upper bound prediction error cou...</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  minimum description length principle online se...  MISC\n",
       "1  if underlying model class discrete then total ...  MISC\n",
       "2  mdl general one only loss bounds finite but ex...  MISC\n",
       "3  we show this even case if model class contains...  AIMX\n",
       "4  we derive new upper bound prediction error cou...  OWNX"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MISC                   1807\n",
       "OWNX                    861\n",
       "AIMX                    190\n",
       "CONT                    168\n",
       "BASE                     61\n",
       "MISC--the                 6\n",
       "MISC--in                  4\n",
       "AIMX--on                  4\n",
       "OWNX--after               2\n",
       "OWNX                      2\n",
       "MISC--for                 2\n",
       "CONT--these               2\n",
       "MISC--on                  2\n",
       "OWNX--we                  2\n",
       "MISC--several             2\n",
       "MISC--specifically,       2\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_1 = training_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>minimum description length principle online se...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if underlying model class discrete then total ...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdl general one only loss bounds finite but ex...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we show this even case if model class contains...</td>\n",
       "      <td>AIMX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we derive new upper bound prediction error cou...</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  minimum description length principle online se...  MISC\n",
       "1  if underlying model class discrete then total ...  MISC\n",
       "2  mdl general one only loss bounds finite but ex...  MISC\n",
       "3  we show this even case if model class contains...  AIMX\n",
       "4  we derive new upper bound prediction error cou...  OWNX"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_1['text'] = [str(word_tokenize(entry)) for entry in copy_1['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ['minimum', 'description', 'length', 'principl...\n",
      "1    ['if', 'underlying', 'model', 'class', 'discre...\n",
      "2    ['mdl', 'general', 'one', 'only', 'loss', 'bou...\n",
      "3    ['we', 'show', 'this', 'even', 'case', 'if', '...\n",
      "4    ['we', 'derive', 'new', 'upper', 'bound', 'pre...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(copy_1['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X_1, Test_X_1, Train_Y_1, Test_Y_1 = model_selection.train_test_split(copy_1['text'],copy_1['label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the vectorizer => transformer => classifier easier to work with, \n",
    "# we will use Pipeline class in Scilkit-Learn that behaves like a compound classifier\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(Train_X_1, Train_Y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(Test_X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7628205128205128\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, Test_Y_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import logging\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=42,\n",
       "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(Train_X_1, Train_Y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(Test_X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8087606837606838\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, Test_Y_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
